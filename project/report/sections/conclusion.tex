\section{Conclusion}
Our empirical research has proven several different key points, the efficacy of different reinforcement learning methods, as well as how useful and effective Neural Networks were on the generalization of an agent in an MDP environment.
\subsection{Deep Q-Learning}
Deep Q-Learning was shown to be effective against many agents, indicating that it generalized, likely due to the prescence of a Neural Network. This goes against the conclusion in \citet{ree13} that Deep Q-Networks were not ideal at generalization. The only indication that this lack of generalization occurred is during selfplay training, as this trained the network with a moving target, rather than random moves.

It is noted that Deep Q-Networks appear to be no less viable than Deep-SARSA and the minimal winrate of Deep-SARSA can be attributed to randomness. However, this may not be the case for larger games played, though this difference appears to be at most a factor of around 5\%.

Limitations of simple Deep Q-Networks is likely do to overestimation bias as stated in \citet{8939117} with solution being through the use of double Q-learning, where an additional Q-network models is used to reduce this bias. Other important Q-networks to explore could have been Dueling Q-networks such as in \citet{DBLP:journals/corr/abs-2106-14642}, where a network represent the components of the Q-function, specifically the value function $V(s)$ and the advantage $A*()$ more accurately, increasing the stability of the algorithm.

\subsection{Deep SARSA}
In this project we experimented with an on-policy method, Deep Reinforcement Learning based on SARSA (Deep SARSA). SARSA learning has some advantages when being applied to decision making problems. It makes learning process more stable and is more suitable to some complicated systems. Given these facts,  Deep SARSA can be used to solve the control problems of video games \cite{7849837} , and for board games like Reversi. Our experiments have shown that Deep SARSA agents performs better that random player, and it as performant as Deep Q-Learning. If we had more time we would work on improving the run-time complexity of the Deep SARSA implementation.